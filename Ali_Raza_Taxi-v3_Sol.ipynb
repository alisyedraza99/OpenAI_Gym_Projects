{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Ali_Raza-P1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alisyedraza99/OpenAI_Gym_Projects/blob/main/Ali_Raza_Taxi-v3_Sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I1ClKylcIR2"
      },
      "source": [
        "# P1: Solve the OpenAI Gym [Taxi V3](https://gym.openai.com/envs/Taxi-v3/) Environment\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGsieww5cIR4"
      },
      "source": [
        "## Introduction\n",
        "[OpenAI Gym](https://gym.openai.com/docs/) is a framework that provides RL environments of varying complexity with the same standard API making it easy to develop and benchmark RL algorithms. The [Taxi-V3](https://gym.openai.com/envs/Taxi-v3/) environmnet present a simple, text environment where actions and state (observations) are both discrete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyLS_7-rcIR5"
      },
      "source": [
        "# Imports\n",
        "import gym\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import clear_output\n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78wYkZcDcIR6"
      },
      "source": [
        "The `gym.make()` API can be used to spawn any of the available environments by passing its full name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9obNe81cIR6"
      },
      "source": [
        "taxi = gym.make('Taxi-v3')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quvYIsmEcIR6"
      },
      "source": [
        "The Taxi environment has 500 states and 6 possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBudl85ucIR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "67361dca-15c9-4bb1-acb4-9907da6d9351"
      },
      "source": [
        "taxi.action_space"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbm2PW1scIR7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f9b8793f-52f7-4513-edc1-2958a21c60d8"
      },
      "source": [
        "taxi.observation_space"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(500)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whF0OORucIR8"
      },
      "source": [
        "The task and reward structure are described in the [documentation](https://github.com/openai/gym/blob/a5a6ae6bc0a5cfc0ff1ce9be723d59593c165022/gym/envs/toy_text/taxi.py#L25)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzZ9FEFTcIR8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31157704-ef07-4cc5-ddec-f9d2fc3b9580"
      },
      "source": [
        "taxi.reset()\n",
        "render = taxi.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| :\u001b[43m \u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "thKUAVZb9YBt",
        "outputId": "02606ccd-7cae-4b63-b363-13a5d7f6665d"
      },
      "source": [
        "state, reward, done, info = taxi.step(taxi.action_space.sample())\n",
        "taxi.P[state]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 228, -1, False)],\n",
              " 1: [(1.0, 28, -1, False)],\n",
              " 2: [(1.0, 128, -1, False)],\n",
              " 3: [(1.0, 108, -1, False)],\n",
              " 4: [(1.0, 128, -10, False)],\n",
              " 5: [(1.0, 128, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z6oY9gJNARZH",
        "outputId": "9a09d0e7-c79d-4a46-f654-c69d38db2892"
      },
      "source": [
        "state, reward, done, info = taxi.step(2)\n",
        "state"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km1MUv15dZjS"
      },
      "source": [
        "## 1\n",
        "***Describe the methods and variables in the class DiscreteEnv which is the parent class of the Taxi V3 class.***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2e7oYwdgi7N"
      },
      "source": [
        "*Attributes/Variables*: \\\n",
        "\n",
        "nS: number of states \\\n",
        "nA: number of actions \\\n",
        "P: a directory of lists representing transitions/rewards table where keys are the states and value is a dictionry with all actions as keys and their values ares lists with action results. \\\n",
        "isd: a list of the initial state distribution of length nS \\\n",
        "action_space: the discrete valid actions that the agent can take. \\\n",
        "observation_space: discrete valid structure of the observation of the enviroment. \\\n",
        "\n",
        "*Methods*: \\\n",
        "seed(): set a seed to reproduce the same results.\\\n",
        "reset(): reset the enviroment to an initial state.\\\n",
        "step(a): returns the current state, reward, if done and probability after taking action a."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp689A2Hdvmp"
      },
      "source": [
        "## 2\n",
        "***Describe the methods and variables in the Taxi V3 class.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBSlbHN4g5Rp"
      },
      "source": [
        "*Attributes/Variables*:\\\n",
        "desc: The visual structure of the enviorment map stored as a np array. \\\n",
        "locs: the indices of the 4 destination locations. \\\n",
        "\n",
        "*Methods*:\\\n",
        "encode: return the state index given the row, col, destination index and passenger location. \\\n",
        "decode: Return the row, col, destination index and passenger location given the state index. \\\n",
        "render: Draw the current observation of the taxi envrionment. \\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6aSHvCtdy_3"
      },
      "source": [
        "## 3\n",
        "***Describe the Taxi V3 environment, its actions, states, reward structure and the rationale behind such a reward structure.***\n",
        "\n",
        "There are 4 designated destionation + in the taxi, where the passenger can be.\n",
        "\n",
        "0. R(ed)\n",
        "1. G(reen)\n",
        "2. Y(ellow)\n",
        "3. B(lue)\n",
        "4. Taxi\n",
        "  \n",
        "The taxi can be in one of 25 positions\n",
        "The passenger can be in 5 locations\n",
        "There are 4 set locations\n",
        "Therefore, 25x5x4=500 discrete states.\n",
        "\n",
        "The available actions are:\n",
        "move south, move north, move east, move west, pickup passenger, drop-off passenger.\n",
        "\n",
        "Reward of -1 for every step, other than correct drop-off which is +20 and illegal pickup or drop-off which is -10. This reward structure is set such that taking the shortest route while avoiding illegal actions will result in the highest return. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P1OEVQAeASa"
      },
      "source": [
        "## 4\n",
        "*Train an algorithm to achieve a 100-episode average reward with a 5th percentile of 7.2 or higher and a 95th percentile of 8.2 or higher on the last 1000 episodes.* \n",
        "\n",
        "## 5\n",
        "*The algorithm should be able to perform pick-ups and dropoffs with zero penalties over 1000 episodes.* \n",
        "\n",
        "## 6\n",
        "*Document your solution including all hyper parameters and how those hyperparameters were selected.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5v9Nn-JKEPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1f48ded5-ac67-47a7-ec1f-8db48dc487df"
      },
      "source": [
        "# This cell is for experimentation. Exploring ways to print \n",
        "# the render and understand enviroment.\n",
        "# References used in this cell:\n",
        "# IPython To clear ouptut (using so we can view renders like animations):\n",
        "#   https://stackoverflow.com/questions/24816237/ipython-notebook-clear-cell-output-in-code\n",
        "# OpenAI Gym to understand basics:\n",
        "#   https://gym.openai.com/docs/\n",
        "\n",
        "steps = 0\n",
        "taxi.reset()\n",
        "for i_episode in range(1):\n",
        "  state = taxi.reset()\n",
        "  for t in range(1000):\n",
        "    clear_output(wait=True)\n",
        "    taxi.render()\n",
        "    print(state)\n",
        "    print(steps)\n",
        "    action = taxi.action_space.sample()\n",
        "    state, reward, done, info = taxi.step(action)\n",
        "    steps += 1\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps. Last action: {}\".format(t+1, taxi.lastaction))\n",
        "        break\n",
        "    time.sleep(0.01)\n",
        "taxi.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "|\u001b[43m \u001b[0m: | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "106\n",
            "199\n",
            "Episode finished after 200 timesteps. Last action: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "nxJ15eAeH0eu",
        "outputId": "0e835378-0d51-4b34-9f0c-9770a52a77c2"
      },
      "source": [
        "#Begin Implementation\n",
        "# Create Q table\n",
        "Q = pd.DataFrame.from_dict({s:{a:0 for a in range(taxi.action_space.n)} for s in range(taxi.observation_space.n)}, orient='index')\n",
        "Q"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0  1  2  3  4  5\n",
              "0    0  0  0  0  0  0\n",
              "1    0  0  0  0  0  0\n",
              "2    0  0  0  0  0  0\n",
              "3    0  0  0  0  0  0\n",
              "4    0  0  0  0  0  0\n",
              "..  .. .. .. .. .. ..\n",
              "495  0  0  0  0  0  0\n",
              "496  0  0  0  0  0  0\n",
              "497  0  0  0  0  0  0\n",
              "498  0  0  0  0  0  0\n",
              "499  0  0  0  0  0  0\n",
              "\n",
              "[500 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "I2xEyZKDLGO-",
        "outputId": "c67132e4-21c0-4136-c097-17e0df8f033c"
      },
      "source": [
        "## Hyperparameters were selected through trial and error\n",
        "## epsilon 0.7 (no decay) with gamma 0.9 alpha 0.1 seemed to get good results in 3000 episodes\n",
        "## Potentially because of buggy run ->epsilon 1.001 (0.999 decay) with gamma 0.9 alpha 0.001 did not do as well.\n",
        "## epsilon 1 (decay 0.999) with gamma 0.9 and alpha 0.1 seemed to get good results but some runs reached cumulative reward of -200 \n",
        "## issues with 1 epsilone (.999) decay alpah 0.01 and gamma 0.9\n",
        "\n",
        "## Final hyperparams - using less than 10000 ep did not see convergence\n",
        "n_episodes = 20000\n",
        "epsilon = 0.7\n",
        "min_epsilon = 0.7\n",
        "epsilon_decay = 1#0.999\n",
        "gamma = 0.9\n",
        "alpha = 0.1\n",
        "\n",
        "# testing with numpy to see if there is any performance gain\n",
        "Q = np.zeros([taxi.observation_space.n, taxi.action_space.n])\n",
        "\n",
        "actions = [0, 1, 2, 3, 4, 5]\n",
        "#default_probs = np.asarray([epsilon/taxi.action_space.n]*taxi.action_space.n,dtype=np.float)\n",
        "\n",
        "for i in range(n_episodes):\n",
        "  render_cnt = 0\n",
        "  epsilon = max(epsilon,min_epsilon)\n",
        "  epsilon *= epsilon_decay\n",
        "  done = False\n",
        "  state_0 = taxi.reset()\n",
        "  while not done:\n",
        "    # Find epsilon greedy action\n",
        "    action_probs = np.asarray([epsilon/taxi.action_space.n]*taxi.action_space.n,dtype=np.float)\n",
        "    greedy_action_index = np.argmax(Q[state_0])\n",
        "    action_probs[greedy_action_index] += 1-epsilon\n",
        "    action_0 = np.random.choice(actions,p=action_probs)\n",
        "\n",
        "    # Take action_t\n",
        "    state_1, reward, done, info = taxi.step(action_0)\n",
        "\n",
        "    # update Q\n",
        "    curr_q_value = Q[state_0, action_0]\n",
        "    max_a1 = np.max(Q[state_1])\n",
        "    Q[state_0, action_0] = curr_q_value + alpha*(reward + (gamma * max_a1) - curr_q_value)\n",
        "\n",
        "    # update current state\n",
        "    state_0 = state_1\n",
        "\n",
        "    if (i+1)%50 == 0:\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Episode: {i}\")\n",
        "\n",
        "\n",
        "Q"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 19999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ],\n",
              "       [-0.58568212,  0.4603532 , -0.58568212,  0.4603532 ,  1.62261467,\n",
              "        -8.5396468 ],\n",
              "       [ 4.348907  ,  5.94323   ,  4.348907  ,  5.94323   ,  7.7147    ,\n",
              "        -3.05677   ],\n",
              "       ...,\n",
              "       [ 7.71469975,  9.683     ,  7.71469668,  5.9432297 , -1.28530025,\n",
              "        -1.28530008],\n",
              "       [ 1.62261462,  2.9140163 ,  1.62261418,  2.91401621, -7.37738544,\n",
              "        -7.37738534],\n",
              "       [14.3       , 11.87      , 14.3       , 17.        ,  5.3       ,\n",
              "         5.3       ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d9n-9p1lbiwD",
        "outputId": "03f2e7fb-0734-4833-ea2b-699b13f49a3a"
      },
      "source": [
        "# Visualizing episodes like an animation\n",
        "\n",
        "actions = ['s', 'n', 'e', 'w', 'pickup', 'drop off']\n",
        "episodes = 2\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = taxi.reset()\n",
        "    done = False\n",
        "    ep_cumul_r = 0\n",
        "\n",
        "    while not done:\n",
        "      action = np.argmax(Q[state])\n",
        "      state, reward, done, info = taxi.step(action)\n",
        "      clear_output(wait=True)\n",
        "      taxi.render()\n",
        "      ep_cumul_r += reward\n",
        "\n",
        "      print(\"Prev Action: {}, Curr state: {}, Return: {}, Penalties: {}\".format(actions[action], curr_state, ep_cumul_r, penalties))\n",
        "      time.sleep(0.2)\n",
        "    \n",
        "    print(\"Episode finished with total reward {}\".format(ep_cumul_r))\n",
        "    time.sleep(1)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "Prev Action: drop off, Curr state: 0, Return: 8, Penalties: 0\n",
            "Episode finished with total reward 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7K2yEsfxVxdY",
        "outputId": "bcd15971-f44b-4961-e818-d839d9290c52"
      },
      "source": [
        "# Test performance using trained Q table for 1000 episodes\n",
        "\n",
        "actions = ['s', 'n', 'e', 'w', 'pickup', 'drop off']\n",
        "total_batches = 10\n",
        "batch = 100\n",
        "avg_cumul_reward = [0]*total_batches\n",
        "penalties = 0\n",
        "\n",
        "for i in range(total_batches):\n",
        "  for j in range(batch):\n",
        "    curr_state = taxi.reset()\n",
        "    done = False\n",
        "    ep_cumul_r = 0\n",
        "    \n",
        "    while not done:\n",
        "      action = np.argmax(Q[curr_state])\n",
        "      curr_state, reward, done, info = taxi.step(action)\n",
        "      ep_cumul_r += reward\n",
        "\n",
        "      if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    avg_cumul_reward[i] += ep_cumul_r\n",
        "    if (j%50 == 0):\n",
        "      print(\"Episode finished with total reward {}\".format(ep_cumul_r))\n",
        "\n",
        "  avg_cumul_reward[i] /= batch\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 10\n",
            "Episode finished with total reward 6\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 7\n",
            "Episode finished with total reward 10\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 11\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 7\n",
            "Episode finished with total reward 7\n",
            "Episode finished with total reward 14\n",
            "Episode finished with total reward 12\n",
            "Episode finished with total reward 7\n",
            "Episode finished with total reward 11\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 8\n",
            "Episode finished with total reward 11\n",
            "Episode finished with total reward 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qcnx-ceXRQGs",
        "outputId": "ad142305-824a-4f90-b75f-5caf1f6d1566"
      },
      "source": [
        "avg_cumul_reward.sort()\n",
        "print(avg_cumul_reward)\n",
        "\n",
        "n_95 = (95/100)*total_batches\n",
        "n_5 = (5/100)*total_batches\n",
        "\n",
        "print(\"95th Percentile: {}\\n5th Percentile:{}\"\\\n",
        "      .format(avg_cumul_reward[int(n_95)],avg_cumul_reward[int(n_5)] ))\n",
        "print(\"Penalties: {}\".format(penalties))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7.63, 7.63, 7.8, 7.88, 7.98, 7.99, 8.27, 8.34, 8.34, 8.44]\n",
            "95th Percentile: 8.44\n",
            "5th Percentile:7.63\n",
            "Penalties: 0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}